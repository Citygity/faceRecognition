{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\test\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import dlib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation  import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.cross_validation  import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用dlib库进行人脸检测\n",
    "cascPath = \"haarcascade_frontalface_default.xml\"\n",
    "dataPath='D:\\\\code\\\\pycode\\\\faceRecognition\\\\database\\\\'\n",
    "patchPath='D:\\\\code\\\\pycode\\\\faceRecognition\\\\faceData\\\\'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "for personFolder in os.listdir(dataPath):\n",
    "    personPath=os.path.join(dataPath,personFolder)\n",
    "    for fileName in os.listdir(personPath):\n",
    "        #遍历文件夹下所有图片\n",
    "        image = cv2.imread(os.path.join(personPath,fileName))\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        #将图片转为灰度模式\n",
    "        faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30, 30))\n",
    "        if(len(faces)==1):\n",
    "            x,y,w,h=list(faces[0])\n",
    "            facePatch=image[y:y+h,x:x+w]\n",
    "            if(not os.path.exists(os.path.join(patchPath,personFolder))):\n",
    "                os.mkdir(os.path.join(patchPath,personFolder)) \n",
    "            #保存所有检测到的人脸\n",
    "            cv2.imwrite(os.path.join(patchPath+personFolder,fileName),facePatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用ImageDataGenerator进行数据增强\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "dataPath='D:\\\\code\\\\pycode\\\\faceRecognition\\\\database\\\\'\n",
    "patchPath='D:\\\\code\\\\pycode\\\\faceRecognition\\\\faceData1\\\\'\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "#        width_shift_range=0.2,\n",
    "#        height_shift_range=0.2,\n",
    "        rescale=None,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        data_format='channels_last',\n",
    "        cval=0,\n",
    "        #channel_shift_range=0,\n",
    "        vertical_flip=False)\n",
    "for personFolder in os.listdir(dataPath):\n",
    "    personPath=os.path.join(dataPath,personFolder)\n",
    "    for fileName in os.listdir(personPath):\n",
    "        image = cv2.imread(os.path.join(personPath,fileName))\n",
    "        dets = detector(image, 1)\n",
    "        for i, d in enumerate(dets):\n",
    "            print(\"person{} face{} Detection {}: Left: {} Top: {} Right: {} Bottom: {}\".format(personFolder,fileName[-5:-4],i, d.left(), d.top(), d.right(), d.bottom()))\n",
    "            facePatch=image[d.top():d.bottom(),d.left():d.right()]\n",
    "            cv2.imwrite(os.path.join(patchPath+personFolder,fileName),facePatch)\n",
    "            \n",
    "            facePatch=img_to_array(facePatch)\n",
    "            facePatch = facePatch.reshape((1,) + facePatch.shape)\n",
    "            if(not os.path.exists(os.path.join(patchPath,personFolder))):\n",
    "                os.mkdir(os.path.join(patchPath,personFolder)) \n",
    "                \n",
    "            i = 0\n",
    "            for batch in datagen.flow(facePatch, batch_size=5,\n",
    "                                      save_to_dir=None#os.path.join(patchPath,personFolder), \n",
    "                                      #save_prefix=personFolder, \n",
    "                                      #save_format='jpeg'\n",
    "                                      ):\n",
    "                cv2.imwrite(os.path.join(patchPath+personFolder,fileName[:-4]+str(i)+'.JPG'),batch[0].astype(int))\n",
    "                i += 1\n",
    "                if i > 5:\n",
    "                    break  # 否则生成器会退出循环\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_ [9.30563394e+01 2.59330455e+01 1.41939514e+01 8.40064569e+00\n",
      " 7.82357035e+00 6.42211011e+00 6.13298868e+00 4.03662275e+00\n",
      " 3.64808265e+00 2.95069853e+00 2.72460899e+00 2.18832621e+00\n",
      " 2.07116385e+00 1.88104333e+00 1.71905881e+00 1.66628237e+00\n",
      " 1.39929170e+00 1.25982322e+00 1.14435455e+00 1.07452002e+00\n",
      " 1.02019881e+00 9.84561914e-01 9.21918097e-01 8.95024320e-01\n",
      " 8.41218152e-01 7.96008563e-01 7.83868748e-01 7.36044410e-01\n",
      " 7.10644629e-01 6.59112705e-01 6.50906526e-01 6.01274336e-01\n",
      " 5.80219340e-01 5.63628472e-01 5.34013969e-01 5.15436771e-01\n",
      " 4.84377142e-01 4.70503563e-01 4.55146570e-01 4.42822087e-01\n",
      " 4.14670610e-01 4.00608457e-01 3.95222584e-01 3.87632994e-01\n",
      " 3.74686138e-01 3.64963239e-01 3.54545247e-01 3.46966623e-01\n",
      " 3.34975085e-01 3.32642584e-01 3.09766357e-01 3.07493404e-01\n",
      " 3.03037957e-01 2.94842128e-01 2.89603262e-01 2.87722256e-01\n",
      " 2.78269778e-01 2.70458588e-01 2.65288739e-01 2.61039397e-01\n",
      " 2.49694320e-01 2.40479758e-01 2.37044316e-01 2.33929066e-01\n",
      " 2.29528623e-01 2.23202231e-01 2.20289355e-01 2.17317705e-01\n",
      " 2.10280015e-01 2.05395337e-01 2.01627452e-01 1.96499826e-01\n",
      " 1.94397882e-01 1.86758069e-01 1.86223159e-01 1.83701985e-01\n",
      " 1.80240143e-01 1.79189392e-01 1.73917331e-01 1.72949263e-01\n",
      " 1.72512392e-01 1.66310731e-01 1.63431274e-01 1.60687651e-01\n",
      " 1.58768386e-01 1.54309398e-01 1.51635673e-01 1.48606232e-01\n",
      " 1.46322625e-01 1.44660714e-01 1.42237777e-01 1.40201023e-01\n",
      " 1.37819516e-01 1.35983299e-01 1.34485431e-01 1.31694498e-01\n",
      " 1.31123759e-01 1.30482571e-01 1.27223483e-01 1.26177531e-01\n",
      " 1.24738112e-01 1.23012813e-01 1.19460143e-01 1.17592647e-01\n",
      " 1.17360529e-01 1.15137722e-01 1.14855512e-01 1.11484062e-01\n",
      " 1.11013474e-01 1.08888209e-01 1.08271096e-01 1.06903569e-01\n",
      " 1.05642025e-01 1.03939817e-01 1.02112207e-01 1.00211532e-01\n",
      " 9.98845358e-02 9.93520385e-02 9.81699152e-02 9.76829875e-02\n",
      " 9.67560055e-02 9.46421369e-02 9.43966364e-02 9.25837671e-02\n",
      " 9.16120626e-02 9.12724620e-02 9.05703637e-02 8.89635470e-02\n",
      " 8.82525696e-02 8.76912680e-02 8.69521819e-02 8.64371858e-02\n",
      " 8.56607878e-02 8.37418033e-02 8.26028396e-02 8.23406173e-02\n",
      " 8.18520531e-02 8.15624778e-02 7.95851343e-02 7.80371081e-02\n",
      " 7.66184012e-02 7.59159952e-02 7.57302697e-02 7.49686315e-02\n",
      " 7.37645445e-02 7.32319782e-02 7.30817867e-02 7.17121568e-02\n",
      " 7.14664639e-02 7.06414055e-02 7.02400230e-02 6.99497024e-02\n",
      " 6.86951167e-02 6.75000493e-02 6.73982857e-02 6.67243922e-02\n",
      " 6.61164827e-02 6.59972821e-02 6.51990477e-02 6.48833476e-02\n",
      " 6.45288492e-02 6.40850129e-02 6.33592481e-02 6.26156327e-02\n",
      " 6.19530708e-02 6.15355974e-02 6.11025170e-02 6.01074312e-02\n",
      " 5.90661235e-02 5.88148594e-02 5.86012061e-02 5.79717380e-02\n",
      " 5.75171168e-02 5.72134633e-02 5.59902213e-02 5.57052994e-02\n",
      " 5.54128611e-02 5.48001714e-02 5.47292838e-02 5.40375794e-02\n",
      " 5.37101630e-02 5.32363737e-02 5.29120801e-02 5.24727168e-02\n",
      " 5.17804432e-02 5.16283170e-02 5.13833681e-02 5.06541747e-02\n",
      " 5.03909022e-02 5.00415657e-02 4.98307615e-02 4.96325941e-02\n",
      " 4.94277080e-02 4.90590550e-02 4.88129532e-02 4.84818041e-02\n",
      " 4.74752466e-02 4.71696125e-02]\n",
      "pca.n_components_ 198\n"
     ]
    }
   ],
   "source": [
    "#读取所有人脸数据，并使用PCA进行降维\n",
    "trainData=[[]]\n",
    "label=[]\n",
    "dataPath='D:\\\\code\\\\pycode\\\\faceRecognition\\\\faceData1\\\\'\n",
    "for dataLabel in os.listdir(dataPath):\n",
    "    folderPath=os.path.join(dataPath,dataLabel)\n",
    "    for fileName in os.listdir(folderPath):\n",
    "        image=cv2.imread(os.path.join(folderPath,fileName))\n",
    "        gray_image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        imageArray=np.array(cv2.resize(gray_image,(101,101))).reshape([101*101])/255\n",
    "        label.append(int(dataLabel))\n",
    "        if(trainData==[[]]):\n",
    "            trainData = [imageArray]\n",
    "            continue\n",
    "        trainData = np.concatenate((trainData, [imageArray]),axis=0)\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(trainData)\n",
    "#print (\"explained_variance_ratio_\",pca.explained_variance_ratio_)\n",
    "#print (\"explained_variance_\",pca.explained_variance_)\n",
    "print (\"pca.n_components_\",pca.n_components_)\n",
    "label=np.array(label)\n",
    "trainData=pca.transform(trainData)\n",
    "    train_images, test_images, train_labels, test_labels = train_test_split(trainData, label, test_size = 0.1, random_state = random.randint(0, 100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.50      1.00      0.67         1\n",
      "          2       1.00      0.30      0.46        10\n",
      "          3       1.00      0.17      0.29         6\n",
      "          4       1.00      0.25      0.40         4\n",
      "          5       0.00      0.00      0.00         2\n",
      "          6       0.33      0.20      0.25         5\n",
      "          7       0.14      0.50      0.22         2\n",
      "          8       0.33      0.20      0.25         5\n",
      "          9       0.33      0.50      0.40         2\n",
      "         10       0.00      0.00      0.00         3\n",
      "         11       0.50      0.17      0.25         6\n",
      "         12       0.33      1.00      0.50         2\n",
      "         13       0.67      0.40      0.50         5\n",
      "         14       0.00      0.00      0.00         4\n",
      "         15       0.00      0.00      0.00         1\n",
      "         16       0.00      0.00      0.00         1\n",
      "         17       0.00      0.00      0.00         1\n",
      "         18       1.00      0.50      0.67         4\n",
      "         19       0.25      0.20      0.22         5\n",
      "         20       0.50      0.25      0.33         4\n",
      "         21       0.50      1.00      0.67         2\n",
      "         22       0.00      0.00      0.00         2\n",
      "         23       1.00      1.00      1.00         3\n",
      "         24       1.00      0.50      0.67         4\n",
      "         25       1.00      0.33      0.50         3\n",
      "         26       0.00      0.00      0.00         1\n",
      "         27       0.50      0.25      0.33         4\n",
      "         28       0.25      0.67      0.36         3\n",
      "         29       0.50      0.25      0.33         4\n",
      "         30       0.18      0.50      0.27         4\n",
      "         31       0.00      0.00      0.00         6\n",
      "         32       1.00      0.75      0.86         4\n",
      "         33       1.00      0.50      0.67         4\n",
      "         34       0.00      0.00      0.00         2\n",
      "         36       0.40      0.40      0.40         5\n",
      "         37       0.00      0.00      0.00         2\n",
      "         38       1.00      1.00      1.00         1\n",
      "         39       0.25      0.50      0.33         2\n",
      "         40       0.00      0.00      0.00         2\n",
      "         41       0.00      0.00      0.00         2\n",
      "         42       0.50      0.50      0.50         2\n",
      "         43       0.00      0.00      0.00         5\n",
      "         45       0.00      0.00      0.00         3\n",
      "         46       1.00      0.50      0.67         2\n",
      "         47       0.00      0.00      0.00         0\n",
      "         48       1.00      1.00      1.00         3\n",
      "         49       0.67      0.80      0.73         5\n",
      "         50       0.00      0.00      0.00         0\n",
      "         51       0.12      0.50      0.20         2\n",
      "         52       0.50      0.50      0.50         2\n",
      "         53       0.00      0.00      0.00         3\n",
      "         54       0.25      0.67      0.36         3\n",
      "         55       0.67      1.00      0.80         2\n",
      "         56       0.00      0.00      0.00         3\n",
      "         57       0.50      0.20      0.29         5\n",
      "         58       0.43      1.00      0.60         3\n",
      "         59       0.29      0.67      0.40         3\n",
      "         60       0.00      0.00      0.00         1\n",
      "         61       0.00      0.00      0.00         4\n",
      "         62       0.00      0.00      0.00         4\n",
      "         63       0.33      0.50      0.40         2\n",
      "         64       0.00      0.00      0.00         1\n",
      "         65       0.00      0.00      0.00         1\n",
      "         66       0.67      0.40      0.50         5\n",
      "         67       0.33      0.50      0.40         2\n",
      "         68       0.33      0.50      0.40         4\n",
      "         69       0.38      0.60      0.46         5\n",
      "         70       0.00      0.00      0.00         4\n",
      "         71       0.00      0.00      0.00         0\n",
      "         72       0.17      0.25      0.20         4\n",
      "         73       0.00      0.00      0.00         1\n",
      "         74       0.00      0.00      0.00         1\n",
      "         75       0.67      0.29      0.40         7\n",
      "         76       0.57      0.80      0.67         5\n",
      "         77       0.00      0.00      0.00         3\n",
      "         79       0.00      0.00      0.00         1\n",
      "         80       0.00      0.00      0.00         2\n",
      "         81       0.00      0.00      0.00         5\n",
      "         82       0.00      0.00      0.00         1\n",
      "         83       0.11      0.25      0.15         4\n",
      "         84       1.00      0.50      0.67         2\n",
      "         85       0.60      0.50      0.55         6\n",
      "         86       1.00      0.50      0.67         2\n",
      "         87       0.00      0.00      0.00         1\n",
      "         88       1.00      1.00      1.00         3\n",
      "         89       0.67      0.50      0.57         4\n",
      "         90       0.00      0.00      0.00         1\n",
      "         91       1.00      1.00      1.00         2\n",
      "         92       0.00      0.00      0.00         6\n",
      "         94       0.33      0.25      0.29         4\n",
      "         95       0.00      0.00      0.00         1\n",
      "         96       0.25      0.50      0.33         2\n",
      "         97       0.67      0.67      0.67         3\n",
      "         99       1.00      0.75      0.86         4\n",
      "        100       1.00      0.25      0.40         4\n",
      "        101       0.33      0.50      0.40         2\n",
      "        102       0.00      0.00      0.00         3\n",
      "        103       0.00      0.00      0.00         2\n",
      "        104       0.00      0.00      0.00         4\n",
      "        105       1.00      0.50      0.67         2\n",
      "        106       0.00      0.00      0.00         3\n",
      "        107       1.00      0.67      0.80         3\n",
      "        108       1.00      1.00      1.00         2\n",
      "        109       1.00      0.50      0.67         2\n",
      "        110       0.75      1.00      0.86         3\n",
      "        111       1.00      0.50      0.67         2\n",
      "        112       0.00      0.00      0.00         0\n",
      "        113       0.17      1.00      0.29         1\n",
      "        114       1.00      0.67      0.80         3\n",
      "\n",
      "avg / total       0.45      0.35      0.36       323\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\test\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#使用SVM进行分类\n",
    "svc = SVC(gamma='auto')\n",
    "svc.fit(train_images,train_labels)   #训练模型\n",
    "test_pred=svc.predict(test_images)   #用测试数据来做预测\n",
    "print(classification_report(test_labels, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_jobs=111)\n",
    "clf.fit(train_images,train_labels) \n",
    "test_pred=clf.predict(test_images)  #用测试数据来做预测\n",
    "print(classification_report(test_labels,test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 101\n",
    "images = []\n",
    "labels = []\n",
    "MODEL_PATH='D:\\\\code\\\\pycode\\\\faceRecognition\\\\models.h'\n",
    "def read_path(path_name):    \n",
    "    for dir in os.listdir(path_name):\n",
    "        picFolderPath=os.path.join(path_name,dir)\n",
    "        print(dir+' '+str(len(os.listdir(picFolderPath))))\n",
    "        for picname in os.listdir(picFolderPath):\n",
    "            if(picname.endswith('JPG')):\n",
    "                labels.append(int(dir))\n",
    "                image = cv2.imread(os.path.join(picFolderPath,picname))\n",
    "                image=cv2.resize(image,(101, 101), interpolation=cv2.INTER_CUBIC)\n",
    "                images.append(image)\n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从指定路径读取训练数据\n",
    "def load_dataset(path_name):\n",
    "    images,labels = read_path(path_name)    \n",
    "    \n",
    "    #将输入的所有图片转成四维数组，尺寸为(图片数量*IMAGE_SIZE*IMAGE_SIZE*3)\n",
    "    #我和闺女两个人共1200张图片，IMAGE_SIZE为64，故对我来说尺寸为1200 * 64 * 64 * 3\n",
    "    #图片为64 * 64像素,一个像素3个颜色值(RGB)\n",
    "    images = np.array(images)\n",
    "    print(images.shape)    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, path_name):\n",
    "        #训练集\n",
    "        self.train_images = None\n",
    "        self.train_labels = None\n",
    "        \n",
    "        #验证集\n",
    "        self.valid_images = None\n",
    "        self.valid_labels = None\n",
    "        \n",
    "        #测试集\n",
    "        self.test_images  = None            \n",
    "        self.test_labels  = None\n",
    "        \n",
    "        #数据集加载路径\n",
    "        self.path_name    = path_name\n",
    "        \n",
    "        #当前库采用的维度顺序\n",
    "        self.input_shape = None\n",
    "        \n",
    "    #加载数据集并按照交叉验证的原则划分数据集并进行相关预处理工作\n",
    "    def load(self, img_rows = 101, img_cols = 101, \n",
    "             img_channels = 3, nb_classes = 111):\n",
    "        #加载数据集到内存\n",
    "        images, labels = load_dataset(self.path_name)  \n",
    "        train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size = 0.2, random_state = random.randint(0, 100)) \n",
    "        valid_images, test_images, valid_labels, test_labels = train_test_split(test_images, test_labels, test_size = 0.5, random_state = random.randint(0, 100)) \n",
    "        \n",
    "        #train_images, valid_images, train_labels, valid_labels = train_test_split(images, labels, test_size = 0.3, random_state = random.randint(0, 100))        \n",
    "        #_, test_images, _, test_labels = train_test_split(images, labels, test_size = 0.5, random_state = random.randint(0, 100))                \n",
    "        \n",
    "        #当前的维度顺序如果为'th'，则输入图片数据时的顺序为：channels,rows,cols，否则:rows,cols,channels\n",
    "        #这部分代码就是根据keras库要求的维度顺序重组训练数据集\n",
    "        if K.image_dim_ordering() == 'th':\n",
    "            train_images = train_images.reshape(train_images.shape[0], img_channels, img_rows, img_cols)\n",
    "            valid_images = valid_images.reshape(valid_images.shape[0], img_channels, img_rows, img_cols)\n",
    "            test_images = test_images.reshape(test_images.shape[0], img_channels, img_rows, img_cols)\n",
    "            self.input_shape = (img_channels, img_rows, img_cols)            \n",
    "        else:\n",
    "            train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, img_channels)\n",
    "            valid_images = valid_images.reshape(valid_images.shape[0], img_rows, img_cols, img_channels)\n",
    "            test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, img_channels)\n",
    "            self.input_shape = (img_rows, img_cols, img_channels)            \n",
    "            \n",
    "        #输出训练集、验证集、测试集的数量\n",
    "        print(train_images.shape[0], 'train samples')\n",
    "        print(valid_images.shape[0], 'valid samples')\n",
    "        print(test_images.shape[0], 'test samples')\n",
    "    \n",
    "        #我们的模型使用categorical_crossentropy作为损失函数，因此需要根据类别数量nb_classes将\n",
    "        #类别标签进行one-hot编码使其向量化，在这里我们的类别只有两种，经过转化后标签数据变为二维\n",
    "#            train_labels = np_utils.to_categorical(train_labels, nb_classes)                        \n",
    "#            valid_labels = np_utils.to_categorical(valid_labels, nb_classes)            \n",
    "#            test_labels = np_utils.to_categorical(test_labels, nb_classes)                        \n",
    "    \n",
    "        #像素数据浮点化以便归一化\n",
    "        train_images = train_images.astype('float32')            \n",
    "        valid_images = valid_images.astype('float32')\n",
    "        test_images = test_images.astype('float32')\n",
    "        \n",
    "        #将其归一化,图像的各像素值归一化到0~1区间\n",
    "        train_images /= 255\n",
    "        valid_images /= 255\n",
    "        test_images /= 255            \n",
    "        lb=LabelBinarizer().fit(np.array(range(0,nb_classes)))\n",
    "        \n",
    "        self.train_images = train_images\n",
    "        self.valid_images = valid_images\n",
    "        self.test_images  = test_images\n",
    "        self.train_labels = lb.transform(train_labels)\n",
    "        self.valid_labels = lb.transform(valid_labels)\n",
    "        self.test_labels  = lb.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.model = None \n",
    "        self.num_classes = 111\n",
    "        self.weight_decay = 0.0005\n",
    "        self.x_shape = [224,224,3]\n",
    "        \n",
    "    #建立模型\n",
    "    def build_model(self, dataset, nb_classes = 111):\n",
    "        #构建一个空的网络模型，它是一个线性堆叠模型，各神经网络层会被顺序添加，专业名称为序贯模型或线性堆叠模型\n",
    "        self.model = Sequential() \n",
    "        weight_decay=self.weight_decay\n",
    "        #以下代码将顺序添加CNN网络需要的各层，一个add就是一个网络层\n",
    "        self.model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                         input_shape=self.x_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.3))\n",
    "\n",
    "        self.model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.4))\n",
    "\n",
    "        self.model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.4))\n",
    "\n",
    "        self.model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.4))\n",
    "\n",
    "        self.model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        self.model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.4))\n",
    "\n",
    "        self.model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.4))\n",
    "\n",
    "        self.model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        self.model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.4))\n",
    "\n",
    "        self.model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.4))\n",
    "\n",
    "        self.model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Dropout(0.5))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(nb_classes))\n",
    "        self.model.add(Activation('softmax'))\n",
    "\n",
    "        #输出模型概况\n",
    "        self.model.summary()\n",
    "        #训练模型\n",
    "    def train(self, dataset, batch_size = 128, nb_epoch = 500, data_augmentation = False):        \n",
    "        sgd = SGD(lr = 0.01, decay = 1e-6, \n",
    "                  momentum = 0.9, nesterov = True) #采用SGD+momentum的优化器进行训练，首先生成一个优化器对象  \n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=sgd,\n",
    "                           metrics=['accuracy'])   #完成实际的模型配置工作\n",
    "        \n",
    "        #不使用数据提升，所谓的提升就是从我们提供的训练数据中利用旋转、翻转、加噪声等方法创造新的\n",
    "        #训练数据，有意识的提升训练数据规模，增加模型训练量+\n",
    "        if not data_augmentation:            \n",
    "            self.model.fit(dataset.train_images,\n",
    "                           dataset.train_labels,\n",
    "                           batch_size = batch_size,\n",
    "                           nb_epoch = nb_epoch,\n",
    "                           validation_data = (dataset.valid_images, dataset.valid_labels),\n",
    "                           shuffle = True)\n",
    "        #使用实时数据提升\n",
    "        else:            \n",
    "            #定义数据生成器用于数据提升，其返回一个生成器对象datagen，datagen每被调用一\n",
    "            #次其生成一组数据（顺序生成），节省内存，其实就是python的数据生成器\n",
    "            datagen = ImageDataGenerator(\n",
    "                featurewise_center = False,             #是否使输入数据去中心化（均值为0），\n",
    "                samplewise_center  = False,             #是否使输入数据的每个样本均值为0\n",
    "                featurewise_std_normalization = False,  #是否数据标准化（输入数据除以数据集的标准差）\n",
    "                samplewise_std_normalization  = False,  #是否将每个样本数据除以自身的标准差\n",
    "                zca_whitening = False,                  #是否对输入数据施以ZCA白化\n",
    "                rotation_range = 20,                    #数据提升时图片随机转动的角度(范围为0～180)\n",
    "                width_shift_range  = 0.2,               #数据提升时图片水平偏移的幅度（单位为图片宽度的占比，0~1之间的浮点数）\n",
    "                height_shift_range = 0.2,               #同上，只不过这里是垂直\n",
    "                horizontal_flip = True,                 #是否进行随机水平翻转\n",
    "                vertical_flip = False)                  #是否进行随机垂直翻转\n",
    "\n",
    "            #计算整个训练样本集的数量以用于特征值归一化、ZCA白化等处理\n",
    "            datagen.fit(dataset.train_images)                        \n",
    "\n",
    "            #利用生成器开始训练模型\n",
    "            self.model.fit_generator(datagen.flow(dataset.train_images, dataset.train_labels,\n",
    "                                                   batch_size = batch_size),\n",
    "                                     samples_per_epoch = dataset.train_images.shape[0],\n",
    "                                     nb_epoch = nb_epoch,\n",
    "                                     validation_data = (dataset.valid_images, dataset.valid_labels))\n",
    "    def evaluate(self, dataset):\n",
    "        score = self.model.evaluate(dataset.test_images, dataset.test_labels, verbose = 1)\n",
    "        print(\"%s: %.2f%%\" % (self.model.metrics_names[1], score[1] * 100))\n",
    "    def save_model(self, file_path = MODEL_PATH):\n",
    "         self.model.save(file_path)\n",
    "    def load_model(self, file_path = MODEL_PATH):\n",
    "        self.model = load_model(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('D:\\\\code\\pycode\\\\faceRecognition\\\\faceData1\\\\')    \n",
    "dataset.load()\n",
    "model = Model()\n",
    "model.build_model(dataset)\n",
    "model.train(dataset)    \n",
    "model.save_model()\n",
    "model.evaluate(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
